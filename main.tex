\documentclass[12 pt, russian]{article}
\usepackage[T1]{fontenc}
\usepackage{ulem}
\usepackage[utf8]{luainputenc}
\usepackage{geometry}
\usepackage[pdftex]{graphicx}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amstext}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,main=russian]{babel}
\usepackage{setspace}
\usepackage{esint}
\usepackage{comment}
\usepackage{babel}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{fullpage} 
\usepackage{parskip} 
\usepackage{tikz} 
\usepackage{indentfirst}
\title{Курсовая работа Шигина Глеба Сергеевича}
\raggedbottom
\begin{document}
\thispagestyle{empty}
\newtheorem{Thm}{Теорема}[section]
\newtheorem{Lemm}{Лемма}[section]
\newtheorem{Rem}{Замечание}[section]
\newtheorem{Co}{Следствие}[section]
\theoremstyle{definition}
\newtheorem{Exam}{Пример}[section]
\newtheorem{Dfn}{Определение}[section]
\sloppy
\begin{titlepage}
\begin{center}
Московский государственный университет имени~М.~В.~Ломоносова\\
Факультет космических исследований\\
%\centering
%\includegraphics[width=0.3\textwidth]{mechmath.jpg}

\noindent\rule{16cm}{0.4pt}
\vfill

\vspace*{100pt} Курсовая работа\\
студента 402 группы \\
Шигина Глеба Сергеевича \\
\vspace{10pt} {\Large{\textbf{}}\\}
Трансформация данных в GLM моделях
\vspace*{40pt}

\begin{flushright}
Научный руководитель:\\ с.н.с., к.ф.-м.н.\\  Шкляев Александр Викторович\\
\end{flushright}

\vspace*{\fill} Москва, 2022
\end{center}
\end{titlepage}

\hfill
   \normalsize{\tableofcontents} % Вывод содержания
\newpage

\section{Введение}
\label{sec:Intro}
TBA
\section{Трансформация данных}
\label{sec:Tarns}
Существуют классы задач, для которых мы знаем, что математическое ожидание ${\bf E}(Y|X)$ является линейной функцией от $X$. 
Это может быть теория, подкрепленная экспериментальными данными \cite{PhysRevD.17.2875}, либо статистическая информация о данных. Например,
пусть $y_i$ и $x_i$ -- выборки их нормальных распределений со средними $\mu_X,\mu_Y$ соответственно, дисперсиями $\sigma_X,\sigma_Y$ соответственно и
корреляцией $\rho_{XY}$. Тогда можно показать (см. \cite[стр. 550]{Berger2001-pm}), что 
\begin{align}
\label{eq:normal-normal}
y_i\mid x_i \sim N\left( \mu_Y-\rho_{XY}\frac{\sigma_Y}{\sigma_X}\mu_X + \rho_{XY}\frac{\sigma_Y}{\sigma_X}x_i,\sigma_Y(1-\rho^2_{XY}) \right).
\end{align}
Это можно переписать как $y_i\mid x_i \sim N \left (\beta_0 + \beta_1 x_i,\sigma^2 \right)$, где 
\begin{align}
\beta_0 = \mu_Y-\rho_{XY}\frac{\sigma_Y}{\sigma_X}\mu_X, \beta_1 = \rho_{XY}\frac{\sigma_Y}{\sigma_X},\sigma^2 = {\bf D}(Y|X) = \sigma^2_Y (1-\rho_{XY}),
\end{align}
то есть мы получили линейную регрессию $Y$ по $X$:
\begin{align}
    {\bf E}(Y|X=x_i) = \beta_0 + \beta_1 x_i.
\end{align}
Однако, у нас не всегда есть возможность узнать истинную зависимость зависимой переменной от предикторов. 
Нам необходимо понять, какие преобразования бывают и как выбрать среди них наилучшее.

Для удобства на данном этапе ограничимся одним \textbf{предиктором} $X$ и \textbf{зависимой переменной} $Y$.
\subsection{Трансформация только зависимой переменной при помощи inverse response plot}
Предположим, что истинная регрессионная модель $Y$ по $X$ имеет вид:
\begin{align}
    \label{eq:g-regression}
    Y = g(\beta_0 + \beta_1 X + \varepsilon),
\end{align}
где $g$ -- некоторая функция, вообще говоря, нам неизвестная. Модель (\ref{eq:g-regression}) может быть приведена
к линейному виду путем преобразования $Y$ с помощью обратной функции $g^{-1}$:
\begin{align}
    g^{-1}(Y) = \beta_0 + \beta_1 X + \varepsilon.
\end{align}
Например, если $Y = \log (\beta_0 + \beta_1 X + \varepsilon),$ то $g(x) = \log (x) ,$ значит $g^{-1}(x) = \exp (x)$, и $\exp(Y) = \beta_0 + \beta_1 X + \varepsilon.$

В данной работе в качестве способа получения оценки $g^{-1}$ предлагается рассмотреть метод обратного отклика
(\textit{inverse response plot}) с подбором функции из степенного семейства (\textit{power family}), 
включающим в себя семейство преобразований Бокса-Кокса.

\subsubsection{Inverse response plot}
\label{subsec:inv-res-plot}
В работе \cite{Cook-Weisberg} было показано, что если $X$ имеет эллиптически симметричное распределение (что
является менее жестким ограничением, чем нормальность), то $g^{-1}$ можно оценить из scatter plot'а, где
по горизонтальной оси откладываются истинные значения $y$, а по вертикальной -- значения $\hat{y}$, полученные из
регрессии на исходных данных: $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$. Такой график называют
\textbf{inverse response plot}.

\subsubsection*{Пример}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{imgs/inverse-response-ex.pdf}
    \caption{Scatter plot $Y$ от $X$ (слева) и inverse response plot (справа)}\label{fig:inv-res-ex}
\end{figure}
Пусть $X\sim N(0, 1),\ \varepsilon \sim N(0,0.1),\ Y = (3+X+\varepsilon)^3.$ Была построена выборка размера $N=100$. Получившуюся зависимость можно увидеть на рисунке \ref{fig:inv-res-ex}.
Построим линейную регрессию $y$ по $x$ без преобразования данных. Получим некоторые оценки $\hat{y}$. Inverse response plot можно также увидеть на рисунке \ref{fig:inv-res-ex}. Помимо точек,
на графике присутствуют три пунктирные кривые -- они показывают результаты линейных регрессий $\hat{y}$ по $y,\ y^{1/3}$ и $\log(y)$ соответственно. Видно, что кривая $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 y^{1/3}$
наиболее близка к исходным точкам. Это ожидаемо, так как из построения $Y$ следует, что искомое нами преобразование для
Y имеет вид $g^{-1}(Y) = Y^{1/3}$.

\subsubsection{Выбор степенного преобразования}

\textbf{Семейство преобразований (\textit{transformation family})} -- это параметризованное множество преобразований, где каждому значению
параметра (или параметров) отвечает некоторый уникальный представитель семейства.

Одним из таких семейств является \textbf{степенное семейство (\textit{power family})}. Оно определено для положительных $X$ и
имеет вид: 
\begin{align}
    \psi(Y, \lambda) = \begin{cases} Y^\lambda,&\lambda \neq 0\\ \log(Y),&\lambda = 0 \end{cases}.
\end{align}

Семейство параметризовано числом $\lambda$, и значение $\lambda = 0$ принимается не за тождественную единицу
(ведь $Y^0 \equiv 1$), а за логарифмическое преобразование $\log(Y)$. Здесь возникает проблема, затрудняющая работу
с этим семейством -- его представители $\psi(Y,\lambda)$ не являются непрерывным по $\lambda$. Поэтому удобнее работать с так
называемым \textbf{нормированным (или отмасштабированным) степенным семейством (\textit{scaled power family})}:
\begin{align}
    \psi_S(Y, \lambda) = \begin{cases} \dfrac{Y^\lambda - 1}{\lambda},&\lambda \neq 0\\ \log(Y),&\lambda = 0 \end{cases}.
\end{align}
Несложно видеть, что $\psi_S(Y,\lambda)$ и $\psi(Y,\lambda)$ отличаются только преобразованием сдвига и масштаба, и линейная регрессия
будет давать аналогичные результаты при двух этих преобразованиях, отличаться будут только веса предикторов. При этом функция $\psi_S(Y,\lambda)$
непрерывна по $\lambda$, и $\log(Y)$ является естественным представителем семейства, так как $\lim_{\lambda\to 0}\psi_S(y,
\lambda) = \log(y)\ \forall y >0.$

Суммируя все вышесказанное, для нахождения оценки $g^{-1}$ мы рассматриваем модели вида
\begin{align}
    \label{eq:yhat-y}
    {\bf E} (\hat{y}|Y=y) = \hat{\beta}_0 + \hat{\beta}_1 \psi_S(y, \lambda).
\end{align}
При фиксированном $\lambda$ модель (\ref{eq:yhat-y}) представляет собой простую линейную регрессию с предиктором
$\psi_S(y,\lambda)$ и зависимой переменной $\hat{y}$. Оптимальным параметром $\hat{\lambda}$ предлагается считать тот,
который минимизирует \textbf{остаточную сумму квадратов (\textit{residual sum of squares})}:
\begin{align}
    \label{eq:RSSl}
    RSS(\lambda) = \sum\limits_{i=1}^{n}\left(\hat{y}_i - \hat{\beta}_0 - \hat{\beta}_1 \psi_S(y_i,\lambda)\right)^2.
\end{align}

\subsection{Трансформация только зависимой переменной при помощи преобразования Бокса-Кокса}
\label{subsec:boxcox-y}
В своей работе \cite{Box-Cox-1964} Бокс и Кокс рассматривали модифицированное семейство
степенных преобразований:
\begin{align}
    \psi_M(Y, \lambda)= GM(Y)^{1-\lambda}\cdot \psi_S(Y,
    \lambda) = \begin{cases}GM(Y)^{1-\lambda}\cdot  \dfrac{Y^\lambda - 1}{\lambda},&\lambda \neq 0\\ GM(Y)\cdot\log(Y),&\lambda = 0 \end{cases},
\end{align}
где $GM(Y) = \sqrt[n]{y_1 y_2 \ldots y_n}$ -- среднее геометрическое выборки.

Метод Бокса-Кокса основывается на предположении, что для некоторого неизвестного $\lambda$ после преобразования
зависимая переменная $\psi_M(Y,\lambda)$ такова, что $\psi_M(y_i,\lambda)$ -- независимые нормально распределенные сл.в. с
постоянной дисперсией $\sigma^2$ и математическим ожиданием
\begin{align}
    {\bf E}(\psi_M(Y,\lambda) | X=x) = \beta_0 + \beta_1 x.
\end{align}

Из этих предположений предлагается брать такое $\lambda$, которое максимизировало бы правдоподобие.
В нормальной модели логарифм правдоподобия имеет вид:
\begin{multline}
    \log(L) = \ell = -\frac{n}{2}\log(2 \pi) - \frac{n}{2}\log(\sigma^2)-\frac{1}{2\sigma^2}\sum\limits_{i=1}^{n}(y_i - \beta_0 -\beta_1 x_i) = \\
    = -\frac{n}{2}\log(2 \pi) - \frac{n}{2}\log(\sigma^2)-\frac{1}{2\sigma^2}RSS.
\end{multline}
Оценка максимума правдоподобия дает нам $\hat{\sigma}^2 = RSS/n.$ Отсюда получаем:
\begin{align}
    \ell = -\frac{n}{2}\log(2 \pi)-\frac{n}{2}-\frac{n}{2}\log (RSS/n).
\end{align}

Можно показать, что якобиан замены преобразования Бокса-Кокса $\psi_M(Y,\lambda)$ равен 1 при любом $\lambda$.
А значит при фиксированном $\lambda$ после трансформации зависимой переменной вид функции правдоподобия останется прежним:
\begin{align}
    \label{eq:log-like-bc}
    \ell(\lambda) = -\frac{n}{2}\log(2 \pi)-\frac{n}{2}-\frac{n}{2}\log (RSS(\lambda)/n).
\end{align}
Так как только последний член выражения (\ref{eq:log-like-bc}) зависит от $\lambda$, то решение задачи
максимизации правдоподобия $L(\lambda)$ (или логарифма правдоподобия $\ell(\lambda)$) по $\lambda$ эквивалентна задаче минимизации
$RSS(\lambda)$ по $\lambda$:
\begin{align}
    \ell(\lambda) = -\frac{n}{2}\log(2 \pi)-\frac{n}{2}-\frac{n}{2}\log (RSS(\lambda)/n) \to \max\limits_\lambda\Leftrightarrow RSS(\lambda) \to \min\limits_\lambda
\end{align}

Здесь и далее для удобства предлагается под преобразованием Бокса-Кокса понимать $\psi_S(X,\lambda)$ для
предикторов и $\psi_M(Y,\lambda)$ для зависимой переменной. Обозначим его $\psi_*(U, \lambda)$.

\subsubsection*{Пример}
Снова обратимся к сгенерированному примеру из пункта \ref{subsec:inv-res-plot}. Из построения нам известно, что искомое значение
$\lambda$ для преобразования $Y$ равно $0.333$. Сравним два метода, которые мы обсудили ранее:
\begin{itemize}
    \item Inverse response plot выдает оценку $\hat{\lambda} = 0.341$;
    \item Метод Бокса-Кокса выдает оценку $\hat{\lambda} = 0.273$.
\end{itemize}
В этом конкретном примере метод Бокса-Кокса оказался ощутимо дальше реального значения параметра $\lambda$, чем
inverse response plot. Однако, как мы увидим далее, преимущество одного метода над другим не так очевидно.

\subsection{Трансформация только предиктора методом Бокса-Кокса}
Мы снова рассмотрим преобразование Бокса-Кокса, на этот раз определенных для строго положительного $X$:
\begin{align}
    \psi_*(X, \lambda) = \begin{cases} \dfrac{X^\lambda - 1}{\lambda},&\lambda \neq 0\\ \log(X),&\lambda = 0 \end{cases}.
\end{align}
Аналогично пункту \ref{subsec:boxcox-y}, мы рассматриваем модели вида
\begin{align}
    \label{eq:EXY_bc}
    {\bf E} (Y|X=x) = \alpha_0 + \alpha_1 \psi_*(x, \lambda).
\end{align}
При фиксированном $\lambda$ модель (\ref{eq:EXY_bc}) представляет собой простую
линейную регрессию с предиктором $\psi_*(x,\lambda)$ и зависимой переменной $y$.
Аналогично задаче преобразования зависимой переменной, для разных значений $\lambda$ строится регрессия методом наименьших квадратов (МНК-регрессия) и
выбирается то $\hat{\lambda}$, которое минимизирует $RSS(\lambda)$.

В качестве альтернативы можно использовать вариацию метода Бокса-Кокса,
которая пытается сделать распределение преобразованного предиктора $X$ как можно более нормальным.
Заметим, что в этом случае регрессионная модель отсутствует, и метод Бокса-Кокса модифицирован для применения непосредственно к $X$.

\subsection{Трансформация и предиктора, и зависимой переменной}
В случае, когда необходимо трансформировать $Y$ и $X$, существует
несколько альтернативных подходов. Ниже эти будут сформулированы эти подходы,
где необходимо -- предоставлена мотивация подхода, и далее будет проведено практическое сравнение.

Итак, предлагаемые варианты:
\begin{enumerate}
    \item Попарная минимизация $RSS$ предикторов из ${\bf X}$ и преобразование $Y$ с помощью inverse response plot (если необходимо);
    
    Предлагается для каждого предиктора подобрать преобразование Бокса-Кокса, минимизирующее $RSS$ в
    регрессии $Y$ только по этому предиктору. Также, может быть полезно трансформировать $Y$ не после этих действий,
    а до, например, когда возникает гетероскедастичность.

    \item Приведение всех предикторов к нормальному виду и преобразование $Y$ с помощью inverse response plot (если необходимо);
    
    Этот метод основывается на идее, что для нормально распределенных величин существует линейная зависимость (см. начало раздела \ref{sec:Tarns}).

    \item Минимизация определителя матрицы выборочных ковариаций $|V({\bf X})|$ и преобразование $Y$ с помощью inverse response plot (если необходимо);

    Этот вариант является расширением предыдущего, так как будут учитываться попарные корреляции между предикторами. 
    В работе \cite{Velilla-1993} метод Бокса-Кокса распространяется на случай многомерных случайных величин.
    Связь минимизации $|V({\bf X})|$ с решением нашей задачи показана в приложении \ref{appendix-velilla}.
    \item Минимизация $RSS$ одновременно по предикторам и зависимой переменной.
    
    В отличие от предыдущего метода, здесь будет учитываться связь зависимой переменной с предикторами.
\end{enumerate}

Для сравнения подходов будет использоваться графический метод, включающий в себя следующие графики:
\begin{enumerate}
    \item Residuals vs fitted.
    
    Это график остатков регрессии в зависимости от прогноза зависимой переменной. Он позволяет оценить адекватность линейной модели.
    \item Leverage plot.
    
    Рычаг (\textit{leverage}) $h_i$ для $i$-го наблюдения -- это диагональный элемент матрицы $X(X^TX)^{-1}X^T$,
    соответствующий коэффициенту при $y_i$ в прогнозе $\hat{y}_i$ (который представляет собой линейную функцию от $y$).
    У каждого наблюдения по горизонтальной оси откладывается рычаг, а по вертикальной -- стандартизированные остатки
    $$\dfrac{r_i}{\hat{\sigma}\sqrt{1-h_i}}.$$
    Эта стандартизация основана на том, что ${\bf D}r_i = \sigma^2 (1-h_i).$ Полученный график позволяет отследить отдельные значения,
    существенно влияющие на коэффициенты модели.
    \item Scale-location plot. Он позволяет оценить гомоскедастичность. Это график корня из стандартизированных остатков
    $r_i / (\sqrt{1-h_i}RSS/n)$ от прогноза зависимой переменной.

    \item Quantile-Quantile plot. Это график стандартизированных остатков от теоретических квантилей некоторого распределения
    (в нашем случае -- нормального). Он позволяет оценить, насколько стандартизированные остатки соответствуют заданному
    распределению.
\end{enumerate}

На рисунке \ref{pic:diag-plot-ex} представлен пример внешнего вида рассматриваемых графиков.

\begin{figure}[h]
    \label{pic:diag-plot-ex}
    \centering
    \includegraphics[width=1\textwidth]{imgs/diag-plot-expl.pdf}
    \caption{Пример графиков, на которых будет происходить сравнение подходов.}
\end{figure}
\section{Заключение}
\label{sec:End}
TBA

\appendix
\section{Многомерное преобразование Бокса-Кокса}
\label{appendix-velilla}

Пусть у нас есть многомерная сл.в. ${\bf X} = (X_1,X_2,\ldots, X_p)$.
Введем преобразование:
\begin{align}
    \psi_M({\bf X}, \pmb{\lambda}) = (\psi_M(X_1,\lambda_1),\psi_M(X_2,\lambda_2),\ldots,\psi_M(X_p,\lambda_p)).
\end{align}
Предположим, что существует такое $\pmb{\lambda}$, что
\begin{align}
    \psi_M({\bf X}, \pmb{\lambda}) \sim N({\pmb\mu}, \bf{V}),
\end{align}
где ${\bf{V}}$ -- некоторая симметричная положительно определенная матрица, которую мы хотим оценить.
Если $\bf{x}$ -- это наблюдения из распределения $\bf{X}$, то правдоподобие имеет вид
\begin{align}
    L(\lambda) = \prod\limits_{i=1}^n\frac{1}{(2\pi|{\bf V}|)^{1/2}}\exp\left( -\frac{1}{2}(\psi_M({\bf x}_i,\pmb{\lambda})  - \mu)^T {\bf V}^{-1}(\psi_M({\bf x}_i, \pmb{\lambda}) - \mu)) \right),
\end{align}
где $|{\bf V}|$ -- определитель матрицы ${\bf V}$. Тогда логарифм правдоподобия имеет вид:
\begin{multline}
    \label{eq:muldim-loglike}
\log (L(\lambda)) = \ell(\lambda) = -\frac{n}{2}\log (2 \pi) - \frac{n}{2}\log (|{\bf V}|) - \\
 - \frac{1}{2}\sum\limits_{i=1}^n\left( -\frac{1}{2}(\psi_M({\bf x}_i,\pmb{\lambda})  - \mu)^T {\bf V}^{-1}(\psi_M({\bf x}_i, \pmb{\lambda}) - \mu)) \right).
\end{multline}
При фиксированном $\pmb{\lambda}$ это логарифм правдоподобия многомерного нормального распределения. Мы можем найти оценки максимума правдоподобия для
$\pmb\mu$ и $\pmb {\bf V}$:
$$\pmb\mu(\pmb\lambda) = \dfrac{1}{n}\sum\limits_{i=1}^n\psi_M({\bf x}_i,\pmb\lambda); $$
$$ {\bf V}(\pmb\lambda) = \dfrac{1}{n}\sum\limits_{i=1}^n (\psi_M({\bf x}_i,\pmb{\lambda})  - \mu)(\psi_M({\bf x}_i, \pmb{\lambda}) - \mu))^T. $$
Подставляя эти оценки в (\ref{eq:muldim-loglike}), получим:
\begin{align}
    \label{eq:muldim-loglike-final}
    \ell(\pmb\lambda) = -\dfrac{n}{2} - \frac{n}{2}\log (2\pi) - \frac{n}{2}\log (|{\bf V}(\pmb\lambda)|)
\end{align}
Максимизация (\ref{eq:muldim-loglike-final}) по $\pmb\lambda$ равносильна минимизации определителя ${\bf V}(\pmb\lambda)$ по $\pmb\lambda$.
\section{Графики}
\label{appendix-imgs}

\bibliographystyle{apalike}
\bibliography{Biblio}
\end{document}
