\documentclass[12 pt, russian]{article}
\usepackage[T1]{fontenc}
\usepackage{ulem}
\usepackage[utf8]{luainputenc}
\usepackage{geometry}
\usepackage[pdftex]{graphicx}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amstext}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,main=russian]{babel}
\usepackage{setspace}
\usepackage{esint}
\usepackage{comment}
\usepackage{babel}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{fullpage} 
\usepackage{parskip} 
\usepackage{tikz} 
\usepackage{indentfirst}
\title{Курсовая работа Шигина Глеба Сергеевича}
\raggedbottom
\begin{document}
\thispagestyle{empty}
\newtheorem{Thm}{Теорема}[section]
\newtheorem{Lemm}{Лемма}[section]
\newtheorem{Rem}{Замечание}[section]
\newtheorem{Co}{Следствие}[section]
\theoremstyle{definition}
\newtheorem{Exam}{Пример}[section]
\newtheorem{Dfn}{Определение}[section]
\sloppy
\begin{titlepage}
\begin{center}
Московский государственный университет имени~М.~В.~Ломоносова\\
Факультет космических исследований\\
%\centering
%\includegraphics[width=0.3\textwidth]{mechmath.jpg}

\noindent\rule{16cm}{0.4pt}
\vfill

\vspace*{100pt} Курсовая работа\\
студента 402 группы \\
Шигина Глеба Сергеевича \\
\vspace{10pt} {\Large{\textbf{}}\\}
Трансформация данных в GLM моделях
\vspace*{40pt}

\begin{flushright}
Научный руководитель:\\ с.н.с., к.ф.-м.н.\\  Шкляев Александр Викторович\\
\end{flushright}

\vspace*{\fill} Москва, 2022
\end{center}
\end{titlepage}

\hfill
   \normalsize{\tableofcontents} % Вывод содержания
\newpage

\section{Введение}
\label{sec:Intro}
TBA
\section{Трансформация данных}
\label{sec:Theory}
Существуют классы задач, для которых мы знаем, что матожидание 
%нет такого слова
${\bf E}(Y|X)$ является линейной функцией от $X$. 
Это может быть теория, подкрепленная экспериментальными данными
\cite{PhysRevD.17.2875},
%в скобках и вообще не очень ясно на что именно идет ссылка.
%кроме того, ссылка на книгу в целом некорректна, нужно указывать главу или раздел. Исключение -- если ссылка в формате (более подробно ....)
либо статистическая информация о данных. Например,
пусть $y_i$ и $x_i$ -- выборки их нормальных распределений со средними $\mu_X,\mu_Y$ соответственно, дисперсиями $\sigma_X,\sigma_Y$ соответственно и
корреляцией $\rho_{XY}$. Тогда можно показать (см. \cite[стр. 550]{Berger2001-pm}), что 
\begin{align}
\label{eq:normal-normal}
y_i\mid x_i 
%Так иногда пишут, но в общем не комильфо. Лучше пишать честно, что условное распределение y_i при условии x_i является нормальных распределением с ....
%И вообще лучше использовать заглавные буквы для случайных величин и маленькие для реализации 
\sim N\left( \mu_Y-\rho_{XY}\frac{\sigma_Y}{\sigma_X}\mu_X + \rho_{XY}\frac{\sigma_Y}{\sigma_X}x_i,\sigma_Y(1-\rho^2_{XY}) \right).
\end{align}
Это можно переписать как $y_i\mid x_i \sim N \left (\beta_0 + \beta_1 x_i,\sigma^2 \right)$, где 
\begin{align}
\beta_0 = \mu_Y-\rho_{XY}\frac{\sigma_Y}{\sigma_X}\mu_X, \beta_1 = \rho_{XY}\frac{\sigma_Y}{\sigma_X},\sigma^2 = {\bf D}(Y|X) = \sigma^2_Y (1-\rho_{XY}),
\end{align}
то есть мы получили линейную регрессию $Y$ по $X$:
\begin{align}
    {\bf E}(Y|X=x_i) = \beta_0 + \beta_1 x_i.
\end{align}
Однако, у нас не всегда есть возможность узнать истинную зависимость целевой переменной от предикторов, 
и любое преобразование данных, используемое нами, не более чем приближение, которое,
как мы надеемся, является подходящим для рассматриваемой задачи. Это поднимает два важных вопроса:
как выбрать преобразование и подходит ли полученная модель к имеющимся данных?
%Непонятно о чем речь

Для удобства на данном этапе ограничимся одним \textbf{предиктором} $X$ и \textbf{зависимой переменной} $Y$.
\subsection{Трансформация только зависимой переменной при помощи inverse response plot}
Предположим, что истинная регрессионная модель $Y$ по $X$ имеет вид:
\begin{align}
    \label{eq:g-regression}
    Y = g(\beta_0 + \beta_1 X + \varepsilon),
\end{align}
где $g$ -- некоторая функция, вообще говоря, нам неизвестная. Модель (\ref{eq:g-regression}) может быть приведена
к линейному виду, преобразовав 
%путем преобразования
$Y$ с помощью обратной функции $g^{-1}$:
\begin{align}
    g^{-1}(Y) = \beta_0 + \beta_1 X + \varepsilon.
\end{align}
Например, если $Y = \log (\beta_0 + \beta_1 X + \varepsilon),$ то $g(x) = \log (x) ,$ значит $g^{-1}(x) = \exp (x)$, и $\exp(Y) = \beta_0 + \beta_1 X + \varepsilon.$

В данной работе в качестве способа получения оценки $g^{-1}$ предлагается рассмотреть метод обратного отклика
(\textit{inverse response plot}) с подбором функции из степенного семейства (\textit{power family}), 
включающим в себя семейство преобразований Бокса-Кокса.

\subsubsection{Inverse response plot}
\label{subsec:inv-res-plot}
В работе \cite{Cook-Weisberg} было показано, что если $x$
%X?
имеет эллиптически симметричное распределение (что
является менее жестким ограничением, чем нормальность), то $g^{-1}$ можно оценить из точечного графика,
%точечный график -- не очень хорошо
где
по горизонтальной оси откладываются истинные значения $y$, а по вертикальной -- значения $\hat{y}$, полученные из
регрессии на нетрансформированных
%исходных
данных: $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$. Такой график называют
\textbf{inverse response plot}.

\subsubsection*{Пример}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{imgs/inverse-response-ex.pdf}
    \caption{Scatter plot $Y$ от $X$ (слева) и inverse response plot (справа)}\label{fig:inv-res-ex}
\end{figure}
Пусть $X\sim N(0, 1),\ \varepsilon \sim N(0,0.1),\ Y = (3+X+\varepsilon)^3.$ Была построена выборка размера $N=100$. Получившуюся зависимость можно увидеть на Рис.\ref{fig:inv-res-ex}.
%Рисунке
Построим линейную регрессию $y$ по $x$ без преобразования данных. Получим некоторые оценки $\hat{y}$. Inverse response plot можно также увидеть на Рис.\ref{fig:inv-res-ex} 
%рисунке
Помимо точек,
на графике присутствуют три пунктирные кривые -- они показывают результаты линейных регрессий $\hat{y}$ по $y,\ y^{1/3}$ и $\log(y)$ соответственно. 
%Орфография!
Видно, что кривая $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 y^{1/3}$
наиболее близка к исходным точкам. Это ожидаемо, так как из построения $Y$ следует, что искомое нами преобразование для
Y имеет вид $g^{-1}(Y) = Y^{1/3}$.

\subsubsection{Выбор степенного преобразования}

\textbf{Семейство преобразований (\textit{transformation family})} -- это параметризованное множество преобразований, где,
варьируя параметры, мы можем получить любого представителя семейства.
%фраза странная

Одним из таких семейств является \textbf{степенное семейство (\textit{power family})}. Оно определено для положительных $X$ и
имеет вид: 
\begin{align}
    \psi(Y, \lambda) = \begin{cases} Y^\lambda,&\lambda \neq 0\\ \log(Y),&\lambda = 0 \end{cases}.
\end{align}

Семейство параметризовано числом $\lambda$, и значение $\lambda = 0$ принимается не за тождественную единицу
(ведь $Y^0 \equiv 1$), а за логарифмическое преобразование $\log(Y)$. Здесь возникает проблема, затрудняющая работу
с этим семейством -- его представители $\psi(Y,\lambda)$ не являются непрерывным по $\lambda$. Поэтому удобнее работать с так
называемым \textbf{нормированным (или отмасштабированным) степенным семейством (\textit{scaled power family})}:
\begin{align}
    \psi_S(Y, \lambda) = \begin{cases} \dfrac{Y^\lambda - 1}{\lambda},&\lambda \neq 0\\ \log(Y),&\lambda = 0 \end{cases}.
\end{align}
Несложно видеть, что $\psi_S(Y,\lambda)$ и $\psi(Y,\lambda)$ отличаются только преобразованием сдвига и масштаба, и линейная регрессия
будет давать аналогичные результаты при двух этих преобразованиях, отличаться будут только веса предикторов. Также, функция
%При этом
$\psi_S(Y,\lambda)$
непрерывна по $\lambda$, и $\log(Y)$ является естественным представителем семейства, так как $\lim_{\lambda\to 0}\psi_S(Y,
\lambda) = \log(Y).$
%Лучше сюда не ставить случайные величины

Суммируя все вышесказанное, для нахождения оценки $g^{-1}$ мы рассматриваем модели вида
\begin{align}
    \label{eq:yhat-y}
    {\bf E} (\hat{y}|Y=y) = \hat{\beta}_0 + \hat{\beta}_1 \psi_S(y, \lambda).
\end{align}
При фиксированном $\lambda$ модель (\ref{eq:yhat-y}) представляет собой простую линейную регрессию с предиктором
$\psi_S(y,\lambda)$ и зависимой переменной $\hat{y}$. Оптимальным параметром $\hat{\lambda}$ предлагается считать тот,
который минимизирует \textbf{остаточную сумму квадратов (\textit{residual sum of squares})}:
\begin{align}
    \label{eq:RSSl}
    RSS(\lambda) = \sum\limits_{i=1}^{n}\left(\hat{y}_i - \hat{\beta}_0 - \hat{\beta}_1 \psi_S(y_i,\lambda)\right).
\end{align}
%А где квадраты?
\subsection{Трансформация только зависимой переменной при помощи преобразования Бокса-Кокса}
\label{subsec:boxcox-y}
В своей работе \cite{Box-Cox-1964} Бокс и Кокс рассматривали модифицированное семейство
степенных преобразований:
\begin{align}
    \psi_M(Y, \lambda)= GM(Y)^{1-\lambda}\cdot \psi_S(Y,
    \lambda) = \begin{cases}GM(Y)^{1-\lambda}\cdot  \dfrac{Y^\lambda - 1}{\lambda},&\lambda \neq 0\\ GM(Y)\cdot\log(Y),&\lambda = 0 \end{cases},
\end{align}
где $GM(Y) = \sqrt[n]{y_1 y_2 \ldots y_n}$ -- среднее геометрическое выборки.

Метод Бокса-Кокса основывается на предположении, что для некоторого неизвестного $\lambda$ после преобразования
зависимая переменная $\psi_M(Y,\lambda)$ такова, что $\psi_M(y_i,\lambda)$ -- независимые нормально распределенные сл.в. с
постоянной дисперсией $\sigma^2$ и матожиданием
\begin{align}
    {\bf E}(\psi_M(Y,\lambda) | X=x) = \beta_0 + \beta_1 x.
\end{align}

Из этих предположений предлагается брать такое $\lambda$, которое максимизировало бы правдоподобие.
В нормальной модели логарифм правдоподобия имеет вид:
\begin{multline}
    \log(L) = \ell = -\frac{n}{2}\log(2 \pi) - \frac{n}{2}\log(\sigma^2)-\frac{1}{2\sigma^2}\sum\limits_{i=1}^{n}(y_i - \beta_0 -\beta_1 x_i) = \\
    = -\frac{n}{2}\log(2 \pi) - \frac{n}{2}\log(\sigma^2)-\frac{1}{2\sigma^2}RSS
    %не теряем точки
\end{multline}
Оценка максимума правдоподобия дает нам $\hat{\sigma}^2 = RSS/n.$ Отсюда получаем:
\begin{align}
    \ell = -\frac{n}{2}\log(2 \pi)-\frac{n}{2}--\frac{n}{2}\log (RSS/n).
\end{align}
%что за --?

Можно показать, что Якобиан
%с маленькой
замены преобразования Бокса-Кокса $\psi_M(Y,\lambda)$ равен 1 при любом $\lambda$.
А значит при фиксированном $\lambda$ после трансформации зависимой переменной вид функции правдоподобия останется прежним:
\begin{align}
    \label{eq:log-like-bc}
    \ell(\lambda) = -\frac{n}{2}\log(2 \pi)-\frac{n}{2}--\frac{n}{2}\log (RSS(\lambda)/n).
\end{align}
Так как только последний член выражения (\ref{eq:log-like-bc}) зависит от $\lambda$, то решение задачи
максимизации правдоподобия (или логарифма правдоподобия) по $\lambda$ эквивалентна задаче минимизации
$RSS$ по $\lambda$.
%несложно запутаться в том, кто такой RSS здесь

Здесь и далее для удобства предлагается под преобразованием Бокса-Кокса понимать $\psi_S(X,\lambda)$ для
предикторов и $\psi_M(Y,\lambda)$ для зависимой переменной. Обозначим его $\psi_*(U, \lambda)$.

\subsubsection*{Пример}
Снова обратимся к сгенерированному примеру из пункта \ref{subsec:inv-res-plot}. Из построения нам известно, что искомое значение
$\lambda$ для преобразования $Y$ равно $0.333$. Сравним два метода, которые мы обсудили ранее:
\begin{itemize}
    \item Inverse response plot выдает оценку $\hat{\lambda} = 0.341$
    \item Метод Бокса-Кокса выдает оценку $\hat{\lambda} = 0.273$
\end{itemize}
%знаки препинания!
В этом конкретном примере метод Бокса-Кокса оказался ощутимо дальше реального значения параметра $\lambda$, чем
inverse response plot. Однако, как мы увидим далее, преимущество одного метода над другим не так очевидно.

\subsection{Трансформация только предиктора методом Бокса-Кокса}
Мы снова рассмотрим преобразование Бокса-Кокса, на этот раз определенных для строго положительного $X$:
\begin{align}
    \psi_*(X, \lambda) = \begin{cases} \dfrac{X^\lambda - 1}{\lambda},&\lambda \neq 0\\ \log(X),&\lambda = 0 \end{cases}.
\end{align}
Аналогично пункту \ref{subsec:boxcox-y}, мы рассматриваем модели вида
\begin{align}
    \label{eq:EXY_bc}
    {\bf E} (Y|X=x) = \alpha_0 + \alpha_1 \psi_*(x, \lambda).
\end{align}
При фиксированном $\lambda$ модель (\ref{eq:EXY_bc}) представляет собой простую
линейную регрессию с предиктором $\psi_*(x,\lambda)$ и зависимой переменной $y$.
Аналогично задаче преобразования зависимой переменной, для разных значений $\lambda$ строится регрессия методом наименьших квадратов (МНК-регрессия) и
выбирается то $\hat{\lambda}$, которое минимизирует $RSS(\lambda)$.

В качестве альтернативы можно использовать вариацию метода Бокса-Кокса,
которая пытается сделать распределение преобразованного предиктора $X$ как можно более нормальным.
Заметим, что в этом случае регрессионная модель отсутствует, и метод Бокса-Кокса модифицирован для применения непосредственно к $X$.

\subsection{Трансформация и предиктора, и зависимой переменной}

\section{Заключение}
\label{sec:End}
TBA

\bibliographystyle{apalike}
\bibliography{Biblio}
\end{document}
